###########################
# Model (general)
###########################

# Path to model weights (for initialization)
baseline_only: False
backbone: densenet121
backbone_pretrained: True
pretrained_weights_path: "./densenet121-a639ec97.pth"
#init_weights: "/media/userdisk0/pretrained_weights/
# resnet18-5c106cde.pth
# resnet50-19c8e357.pth
# densenet121-a639ec97.pth

###########################
# Model (specific)
###########################
model_name: 'snel'
bottleneck_dim: 512
dropout_rate: 0.5
dropout_type: 'Bernoulli' # or 'Gaussian'

diversity_metric: 'var'
lambda_v: 1
n_ensemble: 5
shapley_q: 0.5

swad_checkpoint_freq: 1000
swad_n_converge: 10
swad_n_tolerance: 6
swad_tolerance_ratio: 0.005

###########################
# Optimization
###########################
# training scheme
peer_training: False

# loss function
loss_name: 'ajs'
loss_gjs_weight: [0.01, 0.99]

# warmup
optim_warmup: False
optim_warmup_loss: 'bce'
optim_warmup_n_epoch: 2

optim_name: "adam"
optim_lr: 0.00001
optim_weight_decay: 1e-4
optim_momentum: 0.9
# hyperparameters for SGD
optim_sgd_dampning: 0
optim_sgd_nesterov: True
optim_rmsprop_alpha: 0.99
# the following also apply to other
# adaptive optimizers like adamw
optim_adam_beta1: 0.9
optim_adam_beta2: 0.999
# staged_lr allows different layers to have
# different lr, e.g. pre-trained base layers
# can be assigned a smaller lr than the new
# classification layer
optim_staged_lr: False
#optim_new_layers: []
optim_base_lr_mult: 0.1
# learning rate scheduler
# AVAI_SCHEDS = ["single_step", "multi_step", "cosine"]
optim_lr_scheduler: "cosine"
# -1 or 0 means the stepsize is equal to max_epoch
optim_stepsize: [20, 30, 40]
optim_gamma: 0.1
optim_max_epoch: 50
# set warmup_epoch larger than 0 to activate warmup training
optim_warmup_epoch: -1
# either linear or constant
optim_warmup_type: "linear"
# constant learning rate when type=constant
optim_warmup_cons_lr: 1e-5
# minimum learning rate when type=linear
optim_warmup_min_lr: 1e-5
# recount epoch for the next scheduler (last_epoch=-1)
# otherwise last_epoch=warmup_epoch
optim_warmup_recount: True